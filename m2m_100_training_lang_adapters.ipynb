{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e1572d-e876-433f-a076-07e0e138f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"adapter-transformers@git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3551c529-fcb2-458d-8e5c-4544876eba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric, load_from_disk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, TrainingArguments\n",
    "from transformers.adapters import AdapterTrainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a7310e-77b6-4eb1-87ac-9e6afd4bcbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef787507-63c0-45ca-ba66-d56f4141dc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1631e252-26e7-4ec8-ac5b-efdd94a678fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7895b82a-ddd2-481b-8932-0907b73462a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ea527",
   "metadata": {},
   "source": [
    "# Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d93d087-e26d-4036-be92-8fe797856c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'm2m100_418M'\n",
    "experiment = 'en-ha-lang-adapter-1'\n",
    "dataset_name = 'data/en-ha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcb15db-3e10-4c2d-a788-9f3d755fa5f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"facebook/{model_name}\")\n",
    "# model = torch.nn.DataParallel(model, device_ids=[2, 3, 4])\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"facebook/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e3b53",
   "metadata": {},
   "source": [
    "# Create adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a979bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : also try with original_ln_after=False, which is more theoretically correct but may not result in best performance\n",
    "enc_config = \"pfeiffer[output_adapter=False,monolingual_enc_adapter=True]\"\n",
    "dec_config = \"pfeiffer[output_adapter=False,monolingual_dec_adapter=True]\"\n",
    "\n",
    "# Add lang adapters\n",
    "model.add_adapter(\"enc_en\", config=enc_config)\n",
    "model.add_adapter(\"dec_ha\", config=dec_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f70631-9a45-4400-a064-c55d61bd4a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.add_adapter(\"enc_indo_euro\", config=enc_config)\\nmodel.add_adapter(\"dec_afro_asiatic\", config=dec_config'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Add lang adapters\n",
    "model.add_adapter(\"enc_indo_euro\", config=enc_config)\n",
    "model.add_adapter(\"dec_afro_asiatic\", config=dec_config\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da91567",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f8621a-91a7-430f-abb8-b24099ba7ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47843c49-de6f-4f0e-b972-33aea21d6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({'train':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_train.csv')),\n",
    "                        'validation':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_dev.csv')),\n",
    "                        'test':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_test.csv'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab027a2-400a-4961-90b4-da9b28449308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ha'],\n",
       "        num_rows: 9818\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'ha'],\n",
       "        num_rows: 1113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e09ae18-7223-405e-a273-4b6b39725f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [example for example in examples[src_lang]]\n",
    "    targets = [example for example in examples[tgt_lang]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b0dc3d-64be-41f0-8527-17fcbb3cc657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2269eff71f4a9280528226958c7970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502e95c76e274712bf882f7a780f8178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee83e5f-9065-43f5-bc23-624fa8635ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9818\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0a643",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ad4b70-d1f7-4407-b135-9ee631f83f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7816c388-e691-4c08-874f-14fc97dac435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c1dc421-b458-4edd-94de-c6b18fdffe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a486406-df4c-4e54-93a5-ba859da8c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.adapters.composition as ac\n",
    "\n",
    "# Activate lang adapters\n",
    "model.train_adapter_pair(ac.Pair(\"enc_en\",\"dec_ha\"))\n",
    "\n",
    "\"\"\"\n",
    "# Activate family adapters\n",
    "encoder_adapters = ac.Stack(\"enc_indo_euro\",\"enc_en\")\n",
    "decoder_adapters = ac.Stack(\"dec_afro_asiatic\",\"dec_ha\")\n",
    "model.train_adapter_pair(ac.Pair(encoder_adapters,decoder_adapters))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a61a2b25-0b90-4074-81c2-f80b6f0e26c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./lang_adapters/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    warmup_steps=1000,\n",
    "    # lr_scheduler_type='constant',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2783c81-d3a5-48b7-a18d-74bc3a01ce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(getattr(model.base_model, \"model_frozen\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a361ae7-3f1f-413a-bda9-1e6fe6ee4899",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akufeldt/miniconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9818\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3080\n",
      "  Number of trainable parameters = 4757760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='309' max='3080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 309/3080 09:22 < 1:24:34, 0.55 it/s, Epoch 2/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.915100</td>\n",
       "      <td>6.831612</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>256.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1113\n",
      "  Batch size = 16\n",
      "/tmp/ipykernel_5822/4083629571.py:25: UserWarning: preds: A wannan,' cikinashinun da'in da, baibai bas basyan basiki ba kuma ba cikin daai basij. kuma ba da'ari da kuma da ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_5822/4083629571.py:26: UserWarning: labels: A yau ma a makarantun hauzar mu akwai dalibai da manyan malamai da suke a matsayin dakarun Basiji, kuma suna alfahari da hakan.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "Saving model checkpoint to ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/adapter_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/pytorch_adapter.bin\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/head_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/pytorch_model_head.bin\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/adapter_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/pytorch_adapter.bin\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/head_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/tokenizer_config.json\n",
      "Special tokens file saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/special_tokens_map.json\n",
      "Deleting older checkpoint [lang_adapters/en-ha-lang-adapter-1/model/checkpoint-1232] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1113\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27388ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapters\n",
    "if not os.path.exists(f'./lang_adapters/{experiment}'):\n",
    "    os.mkdir(f'./lang_adapters/{experiment}')\n",
    "    \n",
    "model.save_adapter(f\"./lang_adapters/{experiment}/encoder_english\", \"enc_en\")\n",
    "model.save_adapter(f\"./lang_adapters/{experiment}/decoder_hausa\", \"dec_ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecfaee-465c-4a3c-ad11-82837d41869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MyCustomWeightsLoader(model)\n",
    "loader.save(f\"./lang_adapters/{experiment}/encoder_english\", \"enc_en\")\n",
    "loader.save(f\"./lang_adapters/{experiment}/decoder_hausa\", \"dec_ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928793b-3fa2-45ab-aec7-3b0634a32869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4e6d2-f884-4a9c-a4ff-c1fa70b55da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_outputs = trainer.predict(tokenized_dataset['test'], forced_bos_token_id=tokenizer.get_lang_id(\"ha\"))\n",
    "test_output_texts = tokenizer.batch_decode(torch.LongTensor(test_outputs.predictions), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024e8a5-7972-4c42-a4e1-dbe23c20b7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdf73f-64e8-40de-b6d8-2ab50b234f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f'./lang_adapters/{experiment}'):\n",
    "    os.mkdir(f'./lang_adapters/{experiment}')\n",
    "\n",
    "with open(f'./lang_adapters/{experiment}/predictions', 'w') as fp:\n",
    "    for translation in test_output_texts:\n",
    "        fp.write(translation + '\\n')\n",
    "fp.close()\n",
    "\n",
    "json.dump(test_outputs.metrics, open(f'./lang_adapters/{experiment}/metrics', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee711a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in adapters (for future reference)\n",
    "\n",
    "#model.load_adapter(f\"/lang_adapters/{experiment}/encoder_english\", config=enc_config)\n",
    "#model.load_adapter(f\"/lang_adapters/{experiment}/decoder_hausa\", config=dec_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
